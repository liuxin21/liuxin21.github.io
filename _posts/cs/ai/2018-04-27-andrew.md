---
layout: post
title: 吴恩达课程
date: 2018-04-27
category: ai
---


接下来我们说明一些在余下课程中，需要用到的一些符号。

**符号定义** ：

$x$：表示一个$n_x$维数据，为输入数据，维度为$(n_x,1)$； 

$y​$：表示输出结果，取值为$(0,1)​$；

$(x^{(i)},y^{(i)})$：表示第$i$组数据，可能是训练数据，也可能是测试数据，此处默认为训练数据； 

$X=[x^{(1)},x^{(2)},...,x^{(m)}]$：表示所有的训练数据集的输入值，放在一个 $n_x×m$的矩阵中，其中$m$表示样本数目; 

$Y=[y^{(1)},y^{(2)},...,y^{(m)}]$：对应表示所有训练数据集的输出值，维度为$1×m$。


The loss function computes the error for a single training example; the cost function is the average of the loss functions of the entire training set.

- dvar: The derivative of a final output variable with respect to various intermediate quantities.

上角标代表example的个数，一共有m个

do not use "rnak 1 array"

eg: `np.random.randn(5)` -> (5,)

应该用： `np.random.randn(1,5) -> (1,5)

Minimizing the loss corresponds with maximizing logp(y|x).


Normalizing rows 的原因：
gradient descent converges faster after normalization.

For example, if $$x = 
\begin{bmatrix}
    0 & 3 & 4 \\
    2 & 6 & 4 \\
\end{bmatrix}$$ then $$\| x\| = np.linalg.norm(x, axis = 1, keepdims = True) = \begin{bmatrix}
    5 \\
    \sqrt{56} \\
\end{bmatrix} $$and        $$ x\_normalized = \frac{x}{\| x\|} = \begin{bmatrix}
    0 & \frac{3}{5} & \frac{4}{5} \\
    \frac{2}{\sqrt{56}} & \frac{6}{\sqrt{56}} & \frac{4}{\sqrt{56}} \\
\end{bmatrix}$$ Note that you can divide matrices of different sizes and it works fine: this is called broadcasting.