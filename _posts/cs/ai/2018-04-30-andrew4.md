---
layout: post
title: 深度学习
date: 2018-04-30
category: ai
---

- Hyperparameter tuning
- Regularization
- Optimization

- Diagnose price and variants 
- Advance optimization algorithms

上角标代表example的个数，一共有m个

* 从train set error， 看 bias
* 从train set error 和 dev set error 的差，看variance.

L2 regularization

Frobenius norm: the sum of square of elements of a matrix

### “Weight decay”:
以前是w-alpha*dw

现在是(1-alpha*lambda/m)*w-alpha*dw
 
Lambda 很大的话，w会变小，z也会变小。当z变小的时候，每一层都会接近linear.

D3 表示一个三层的dropout 向量：

d3 = np.random.rand(a3.shape[0], a3.shape[1])


总结：

dev集的error太大 -> variance太大 -> 需要减少overfitting:

1. Regularization
2. More training data


High bias:

1. Deeper neural network
2. Increase the number of units
3. More test data



